{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta prática iremos apresentar o uso de embeddings. Para isso, você deve primeiro instalar as dependencias usando `pip install -r requirements.txt` (ou `pip3`, dependendo da forma que seu python está instalado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, você deverá baixar os repositorios em português e inglês e salvá-los na pasta `embedding_data` seguindo as seguintes instruções: \n",
    "\n",
    "- [No respositório da USP](http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc) baixe [este arquivo (Glove 100 dimensões)](http://143.107.183.175:22980/download.php?file=embeddings/glove/glove_s100.zip). Ele possui  um pouco mais de 600 mil palavras retiradas de textos de páginas Web tais como a Wikipedia e canais de notícias [(Hartmann et al., 2017)](https://arxiv.org/abs/1708.06025). Descomprima e renomeie o arquivo txt para `glove.pt.100.txt`.\n",
    "\n",
    "- No [repositório de Stanford](https://nlp.stanford.edu/projects/glove/), baixe [este arquivo](http://nlp.stanford.edu/data/glove.6B.zip) use o arquivo . Este arquivo compreende ~400 mil palavras de textos extraidos da Wikipédia e [GigaWord](https://catalog.ldc.upenn.edu/LDC2011T07) [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). Descomprima e salve o arquivo com embeddings de 100 dimensões (nome `glove.6B.100d.txt`) na pasta `embedding_data` renomeando esse arquivo para `glove.en.100.txt`.\n",
    "\n",
    "Como você pode perceber, esta prática demandará um espaço livre em disco de aproximadamente 3GB. Os arquivos estão no seguinte formato: em cada linha, uma palavra e N valores representando o valor em cada uma das N dimensões do embedding desta palavra. Por exemplo, caso as palavras `casa`, `redondel` e `rei` sejam representadas por um embedding de 4 dimensões, uma possível representação seria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "casa 0.12 0.1 0.5 -0.4\n",
    "redondel 0.2 0.1 -0.4 0.5\n",
    "rei 0.1 0.5 -0.1 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `get_embedding`, do arquivo `embeddings/utils.py` é responsável por ler esse arquivo e gerar um dicionário em que a chave é a palavra e o valor é sua representação por meio de embeddings. Para a  representação acima, a saída desta função seria seria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dict_embedding_ex = {\n",
    "                        \"casa\":np.array([0.12,0.1,0.5,-0.4]),\n",
    "                        \"redondel\":np.array([0.2,0.1,-0.4,0.5]),\n",
    "                        \"rei\":np.array([0.1,0.5,-0.1,0.1]),\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa função, também é salvo o objeto criado usando [pickle](https://docs.python.org/3/library/pickle.html), assim, a próxima vez que seja lido o embedding, a leitura será mais rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 1 - obtenção do embedding**: Complete a função `get_embedding` obtendo a palavra e o vetor de embeddings com a dimensão `embeddings_size` substituindo os `None` apropriadamente. O dataset possui algumas incosistencias que você deve considerar ao modificar essas linhas: no dataset em português, a maioria das palavras compostas são separadas por hífen, porém, foi verificado que umas palavras foi separado por espaço. Por caso disso, você deve considerar que as `embeddings_size` últimas posições são os valores de cada dimensão, separados por espaço e, as demais, são a palavra. Sugiro \"brincar\" abaixo com o uso de [índice negativo](https://www.geeksforgeeks.org/python-negative-index-of-element-in-list/) entenda também o [método join](https://www.geeksforgeeks.org/join-function-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pé de moleque\n",
      "'pé de moleque': [ 0.1 -0.5  0.5  0.1 -0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "linha = \"pé de moleque 0.1 -0.5 0.5 0.1 -0.5\"\n",
    "embedding_size = 5\n",
    "arr_line = linha.strip().split()\n",
    "\n",
    "word = \" \".join(arr_line[:-embedding_size])\n",
    "print(word)\n",
    "#colocamos float16 para economizar memória\n",
    "embedding = np.array(arr_line[-embedding_size::], dtype=np.float16)\n",
    "print(f\"'{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o teste unitário abaixo para verificar o funcionamento do `get_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: rei\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute os embeddings em português e ingles. Não se preocupe com as palavras ignoradas: foram algumas inconsistencias no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n",
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from embeddings.utils import get_embedding, plot_words_embeddings\n",
    "\n",
    "str_dataset = \"glove.en.100.txt\"\n",
    "dict_embedding_en = get_embedding(str_dataset)\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "dict_embedding_pt = get_embedding(str_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `plot_words_embeddings` utiliza [Análise de Componentes Principais](https://pt.wikipedia.org/wiki/An%C3%A1lise_de_componentes_principais) (PCA, do inglês Principal Component Analisys) para reduzir cada embedding em 2 dimensões para, logo após, plotar em um grafico a posição dessas palavras de acordo com o embedding. Veja o grafico apresentado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_to_use = {\n",
    "                        \"en\":\n",
    "                            {\"embedding\":dict_embedding_en,\n",
    "                            \"words_to_use\":[\"prince\",\"princess\",\n",
    "                                            \"duchess\", \"duke\", \"countess\", \"marquis\", \n",
    "                                            \"marquise\",\"king\",\"queen\",\n",
    "                                            \"girl\",\"boy\",\"man\",\"woman\",\"child\"]},\n",
    "\n",
    "                        \"pt\":{\"embedding\":dict_embedding_pt,\n",
    "                          \"words_to_use\":[\"principe\",\"rei\",\"rainha\",\"conde\",\"duquesa\",\"duque\",\"condessa\",\n",
    "                           \"marquês\",\"marquesa\",\n",
    "                           \"homem\",\"mulher\",\"princesa\",\"menina\",\"menino\",\"criança\",\n",
    "                           \"garoto\",\"garota\"]}\n",
    "                }\n",
    "\n",
    "language = \"en\"#mude de 'pt' para 'en' para ver em ingles tb!\n",
    "plot_words_embeddings(embeddings_to_use[language][\"embedding\"], \n",
    "                    embeddings_to_use[language][\"words_to_use\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, em português, veja que podemos pensar em dois conceitos claramente divididos: a realeza e o gênero. Pense, neste plano cartesiano: qual eixo corresponde ao conceito de realeza? E o de gênero? Perceba que \"criança\" deveria ter genero neutro - de fato, está mais próximo do zero. Porém, pode haver algum ruído associando a palavra criança ao genero feminino. Isso, em português, pode haver uma explicação, pois utilizamos  o artigo `a`, usado para palavras que remetem ao genero feminino, para se referir a criança. Assim, em português, os artigos podem aproximar uma palavra de genero neutro a um determinado genero.\n",
    "\n",
    "\n",
    "Em inglês, não foi possível verificar tão bem a divisão entre os conceitos de `genero` e `realeza`. Isso pode ocorrer devido a redução de dimensionalidade: os conceitos não necessariamente correspondem a um eixo no plano cartesiano e, mesmo se corresponder, ao mapear itens com $n$ dimensões para um plano bidimensional, pode haver perda de informação. Mesmo assim, conseguimos ver a separação entre palavras da realeza e que não são da realeza. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinta-se livre para \"brincar\", alterando/adicionando palavras. Por exemplo, adicione animais. Devido à ambiguidades, ao dataset e à própria redução de dimensionalidade, podem existir palavras que estão erroneamente próximas, se considerarmos o conceito das mesmas,  principalmente se adicionarmos palavras de conceitos muito distintos. Um detalhe: no dataset em português, há uso de palavras compostas e elas estão (geralmente) separadas por hífen. No dataset em inglês não há palavras compostas.\n",
    "\n",
    "Tanto nesta tarefa quanto na próxima você poderá perceber que os embeddings podem carregar preconceitos. Há uma forma de modificar os vetores para eliminar um determinado tipo de preconceito. Por exemplo, nesses embeddings existirão palavras erronemente similares a um determinado genero e, para corrigir, é possível deixar todas as palavras sem distinção pelo genero. Caso queira saber como minimizar esse problema, veja o artigo \"[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)\". O título do artigo se remete a um preconceito descoberto ao usar analogias, que será o próximo tópico desta prática. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de analogias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra caracteristica muito interessante ao usar embedding é a criação de analogias. Por exemplo, na frase `homem está para mulher assim como rei está para...`, fazendo operações com os _embeddings_, muitas vezes é possível chegar na analogia mais provável que, neste caso, seria a palavra `rainha`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 2 - calculo da analogia: ** Nesta atividade, iremos implementar o método `calcula_embedding_analogia` da classe `Analogy`. Essa classe tem acesso ao dicionário de embeddings e a estrutura KDTree, que iremos explicá-la posteriormente. Considerando a frase <span style=\"color:blue\">\"**palavra_x** está para **palavra_y** assim como **assim_como** esta para **palavra_z**\"</span>, o método `calcula_embedding_analogia` recebe como parametro as palavras `palavra_x`, `esta_para` e `assim_ como` e retorna um embedding que, possivelmente, será muito próximo da `palavra_z`. \n",
    "\n",
    "Veja [na aula](https://docs.google.com/presentation/d/1-CggYUA2s7LW7_LcnGv7vlpUGFg9kEWG0j6lWGUnaLI/edit?usp=sharing) como é feito o calculo e, logo após, faça o teste unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.0, -1.0, -1.0, ]\n",
      "[ 12.296875, 53.09375, 30.984375, ]\n",
      "[ -10.96875, -30.90625, -9.6015625, ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2, 3],[-1.2, 3.2, 1.2],[12.2, 31.2, 11.2]], dtype=np.float16)\n",
    "esta_para = np.array([[-3, 0, 1],[11, 56, 32.2],[0, 0.2, 0.4]], dtype=np.float16)\n",
    "assim_como = np.array([[2, 1, 1],[0.1,0.3,0],[1.23, 0.1, 1.2]], dtype=np.float16)\n",
    "\n",
    "for i,x_val in enumerate(x):\n",
    "    arr_embedding = assim_como[i] - x[i] + esta_para[i]\n",
    "    print(\"[\",end=\" \")\n",
    "    for val in arr_embedding:\n",
    "        print(float(val),end=\", \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1. 2. 3.] esta para: [-3.  0.  1.] assim_como: [2. 1. 1.]\n",
      "x: [-1.2  3.2  1.2] esta para: [11.  56.  32.2] assim_como: [1. 2. 1.]\n",
      "x: [12.2 31.2 11.2] esta para: [0.  0.2 0.4] assim_como: [1.23 0.1  1.2 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_calculo_analogia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 3 - busca da palavra mais similar:** O calculo da atividade anterior resultou em um embedding e, agora, precisamos  procuramos a palavra mais próxima a este embedding obtido. Para isso, precisamos de: (1) uma forma eficiente para percorrer os embeddings para descobrir o mais similar; (2) uma métrica de similaridade/distancia; \n",
    "\n",
    "**Como percorrer embeddings?** Para encontrarmos os embeddings similares, uma alternativa seria percorrer todos os vetores de embeddings e encontrar o mais similar. Porém, como estamos trabalhando com centenas de milhares de embeddings, essa operação seria muito custosa. Para isso, podemos usar uma estrutura de dados chamada **KDTree**. KDtree é uma arvore que organiza dados espaciais de tal forma que conseguimos alcançar elementos similares de forma mais eficiente. Caso esteja interessado em mais detalhes, [veja este video](https://www.youtube.com/watch?v=Glp7THUpGow).\n",
    "\n",
    "**Qual métrica de distancia/similaridade usaremos?**  Já foi demonstrado que esta métrica é eficiente para similaridade entre embeddings é a distancia euclidiana [(Pennington et al., 2015)](https://nlp.stanford.edu/pubs/glove.pdf). A [distancia euclidiana](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_euclidiana) entre dois pontos $p$ e $q$ é calculada por meio do tamanho da linha entre esses pontos. Para um espaço bidimensional, considerando que os pontos $p$ e $q$ são representados pelas coordenadas $(p_1,p_2)$ e $(q_1,q_2)$, respectivamente, a equação é dada pela seguinte fórmula: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2}$ veja uma representação gráfica: \n",
    "\n",
    "<img width=\"400px\" src=\"img/distancia_euclidiana.svg\">\n",
    "\n",
    "Esta métrica pode ser generalizada para um espaço n-dimensional e o cálculo seria: $d(p,q) = \\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, nesta atividade iremos utilizar [a implementação do kdtree do scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html). Nessa estrutura, é possível armazenar os embeddings e, logo após fazer consultas eficiente para, por exemplo, procurar os k elementos mais próximos. Veja o exemplo abaixo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O ponto [3. 3.] é o 1º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [2. 2.] é o 2º ponto mais próximo de [3, 2] distância: 1.0\n",
      "O ponto [1. 1.] é o 3º ponto mais próximo de [3, 2] distância: 2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np \n",
    "# Estava com o erro 'list' object has no attribute 'size', então fiz o array com a numpy e resolveu :)\n",
    "\n",
    "elementos = np.array([[1,1],\n",
    "                      [2,2],\n",
    "                      [3,3],\n",
    "                      [4,4],\n",
    "                      [5,5],\n",
    "                      [6,6]], dtype=np.float16)\n",
    "\n",
    "#os elementos são passados como parametro na construção do KDTree junto com a métrica \n",
    "#de distancia que iremos usar\n",
    "kdtree = KDTree(elementos,  metric='euclidean')\n",
    "\n",
    "#retorna os 2 elementos mais próximos e sua distancia\n",
    "#como podemos fazer uma consulta por lista de pontos, temos que \n",
    "#passar uma lista de pontos como parametro\n",
    "ponto = [3,2]\n",
    "distancia,pos_mais_prox = kdtree.query([ponto], k=3, return_distance=True)\n",
    "for i,pos in enumerate(pos_mais_prox[0]):\n",
    "    elemento = elementos[pos]\n",
    "    distancia_ponto = distancia[0][i]\n",
    "    print(f\"O ponto {elemento} é o {i+1}º ponto mais próximo de {ponto} distância: {distancia_ponto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, cada embedding pode ser armazenado no KTree para, logo após, obtermos os embeddings mais próximos a um embedding em questão. Não é possível armazenar na estrutura do KDTree a palavra referente a cada embedding representado, por isso, armazenamos essa estrutura como um atributo da classe `KDTreeEmbedding` (arquivo `utils.py`) que armazena também os atributos `pos_to_word` mapeando, para cada posição a palavra correspondente e o atributo `word_to_pos` que faz o oposto: mapeia, para cada palavra, a posição correspondente. Veja no construtor de `KDTreeEmbedding`  como é criado o KDTree. Nela, também será salvo um arquivo com a implementação do KDtree e os atributos `pot_to_word` e `word_to_pos` isso é necessário pois a criação da KDTree é muito custosa.\n",
    "\n",
    "\n",
    "Nesta atividade, você deverá implementar `get_most_similar_embedding` que obtém as $k$ palavras mais similares à palavra (ou embedding) representado pelo parametro `query` por meio do método `query` da KDTree. O parâmetro `query` pode ser a palavra (`string`) ou o proprio embedding (`np.array`). Logo após, implemente também o método `get_embeddings_by_similarity` que utiliza o método `query_radius` ([veja documentação](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree.query_radius)) que retorna todas as palavras que estão em um raio de `max_distance` da palavra alvo especificada pelo parametro `query`. Para ambas as implementações, utiliza-se o método `positions_to_word`, já implementado, para retornar as palavras de acordo com as posições indicadas. Caso haja alguma palavra a ser ignorada em `words_to_ignore` ela será excluída também no método `positions_to_word`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_get_most_similar_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_embeddings_by_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, você pode testar os métodos utilizando os datasets de embeddings. Lembre-se  que o KDTree pode demorar mais de 30 minutos para ser criado na primeira execução de cada idioma. Caso queira testar para o ingles, não esqueça de mudar de `\"kdtree.pt.p\"` para `\"kdtree.en.p\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  3.8529124618382,\n",
       "  3.879822890301186,\n",
       "  4.110205347652384,\n",
       "  4.330550049048636],\n",
       " ['carro', 'veículo', 'caminhão', 'motorista', 'moto'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "str_dataset = \"glove.pt.100.txt\"\n",
    "kdtree_file = \"kdtree.pt.p\"\n",
    "dict_embedding = get_embedding(str_dataset)\n",
    "kdtree = KDTreeEmbedding(dict_embedding, kdtree_file)\n",
    "kdtree.get_most_similar_embedding(\"carro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade 5 - 💞 apresentando as analogias 💞:** Agora você deverá implementar o método `analogia` da classe `Analogy` que deverá utilizar os métodos `calcula_embedding_analogia` e o `get_most_similar_embedding` para retornar as 4 palavras mais prováveis para completar uma determinada analogia, com os parametros indicados. Caso, dentre as 4 palavras, haja uma palavra dos parametro de entrada, a mesma pode ser excluída, retorando menos palavras. Por exemplo, considerando \"**rei** está para **rainha** assim como **homem** está para...\" uma caso uma das palavras de saída para essaa entrada  seja `rainha`, o método poderá retornar 3 palavras (eliminando a palavra rainha). Isso já é considerado no método `get_most_similar_embedding`. Lembre-se que o método `get_most_similar_embedding` é da classe KDTreeEmbedding e a `Analogy`possui o atributo `kdtree_embedding` que é uma instancia da classe `KDTreeEmbedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1. 2. 3.] esta para: [-3.  0.  1.] assim_como: [2. 1. 1.]\n",
      "x: [-1.2  3.2  1.2] esta para: [11.  56.  32.2] assim_como: [ 0.21 12.    1.3 ]\n",
      "x: [12.2 31.2 11.2] esta para: [0.  0.2 0.4] assim_como: [1.23 0.1  1.2 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python -m embeddings.embedding_tests TestEmbeddings.test_analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja as analogias (brinque a vontantade com a representação em português e em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "460000: nikolaeva\n",
      "470000: chi0\n",
      "480000: neuropatológicas\n",
      "490000: atulmente\n",
      "500000: megainvestigação\n",
      "510000: analista-tributário\n",
      "520000: gitirana\n",
      "530000: quidação\n",
      "540000: baios\n",
      "550000: jefa\n",
      "560000: tae-hyun\n",
      "570000: celebuzz\n",
      "580000: heparan\n",
      "590000: palomonte\n",
      "600000: tuymans\n",
      "610000: comaroff\n",
      "620000: jōdai\n",
      "630000: republicanista\n",
      "640000: aglutinar-se\n",
      "650000: colonist\n",
      "660000: fronteia\n",
      "670000: locomoviam-se\n",
      "680000: podlasie\n",
      "690000: tamtert\n",
      "700000: alvalde\n",
      "710000: decoimas\n",
      "720000: holdstock\n",
      "730000: notificou-os\n",
      "740000: sipylum\n",
      "750000: 0000px\n",
      "760000: batumbulan\n",
      "770000: conisania\n",
      "780000: ergoldsbach\n",
      "790000: harlington-straker\n",
      "800000: lanley\n",
      "810000: navigabilidade\n",
      "820000: prolongarse\n",
      "830000: sitophilus\n",
      "840000: vassilko\n",
      "850000: ajuda-pinto\n",
      "860000: canaã£,\n",
      "870000: dewberry\n",
      "880000: fritagem\n",
      "890000: kepple\n",
      "900000: nauticos\n",
      "910000: quartel-central\n",
      "920000: successi\n",
      "Palavras ignoradas: 2\n",
      "brasil está para brasilia assim como...\n",
      "\tperu está para brasilia (ou ['aneura', 'trong', 'lacrima'])\n",
      "\tgana está para aneura (ou ['seraing', 'fene', 'dewsbury'])\n",
      "\tjapão está para yeonpyeong (ou ['lieja', 'pottstown', 'belém-pa'])\n",
      "\tespanha está para logroño (ou ['valladolid', 'kalapa', 'cádiz'])\n",
      "\tindia está para vjm00 (ou ['mailman', 'excursion', 'nsi'])\n",
      "bahia está para salvador assim como...\n",
      "\tacre está para acre (ou ['salvador', 'macapá', 'aracaju'])\n",
      "\talagoas está para salvador (ou ['aracaju', 'maceió', 'alagoas'])\n",
      "\tamapá está para amapá (ou ['macapá', 'salvador', 'amazonas'])\n",
      "\tamazonas está para salvador (ou ['amazonas', 'maceió', 'aracaju'])\n",
      "\tceará está para salvador (ou ['maceió', 'cuiabá', 'aracaju'])\n",
      "\tgoiás está para salvador (ou ['goiânia', 'cuiabá', 'aracaju'])\n",
      "brasil está para feijoada assim como...\n",
      "\titalia está para esparguete (ou ['via-crúcis', 'negundo', 'pana'])\n",
      "\testados-unidos está para cebolada (ou ['portugas', 'atribuindo-os', 'bróculos'])\n",
      "\tinglaterra está para feijoada (ou ['worcestershire', 'falmouth', 'lincolnshire'])\n",
      "\targentina está para feijoada (ou ['retrete', 'esparguete', 'carnico'])\n",
      "\tperu está para feijoada (ou ['frita', 'molho', 'guisado'])\n",
      "homem está para mulher assim como...\n",
      "\tgaroto está para menina (ou ['garota', 'namorada', 'mãe'])\n",
      "\trei está para rainha (ou ['rei', 'princesa', 'esposa'])\n",
      "\tpríncipe está para princesa (ou ['príncipe', 'rainha', 'filha'])\n",
      "\tpai está para filha (ou ['mãe', 'esposa', 'irmã'])\n",
      "\tcavalo está para cavalo (ou ['dama', 'égua', 'carruagem'])\n",
      "\tgarçon está para garçon (ou ['cabeleireira', 'edna', 'pescadora'])\n",
      "grande está para pequeno assim como...\n",
      "\tcheio está para cheio (ou ['armário', 'saco', 'gato'])\n",
      "\talto está para alto (ou ['pequeno', 'baixo', 'comprido'])\n",
      "\tforte está para pequeno (ou ['fraco', 'forte', 'parecido'])\n",
      "\tlargo está para largo (ou ['pequeno', 'beco', 'fronteiro'])\n",
      "pelé está para futebol assim como...\n",
      "\ttyson está para hóquei (ou ['basquetebol', 'campeão', 'tyson'])\n",
      "\tbolt está para futebol (ou ['bolt', 'hóquei', 'atletismo'])\n",
      "\tsenna está para ciclismo (ou ['futebol', 'liga', 'campeonato'])\n",
      "atena está para sabedoria assim como...\n",
      "\tafrodite está para sabedoria (ou ['bondade', 'harmonia', 'compaixão'])\n",
      "\tposeidon está para sabedoria (ou ['inefável', 'sábio', 'bondade'])\n",
      "\tzeus está para sabedoria (ou ['compaixão', 'bondade', 'alma'])\n",
      "\tatena está para sabedoria (ou ['bondade', 'serenidade', 'generosidade'])\n",
      "cruzeiro está para raposa assim como...\n",
      "\tatlético está para raposa (ou ['atlético', 'galo', 'csa'])\n",
      "\tgremio está para gremio (ou ['exquisite', 'arara-azul-de-lear', 'noreña'])\n",
      "\tpalmeiras está para raposa (ou ['palmeiras', 'macaca', 'verdão'])\n",
      "\tcorinthians está para raposa (ou ['timão', 'corinthians', 'galo'])\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import *\n",
    "dict_embedding = get_embedding( \"glove.pt.100.txt\",100)\n",
    "obj_analogy = Analogy(dict_embedding,\"kdtree.pt.p\")\n",
    "\n",
    "\n",
    "dict_analogias = {(\"brasil\",\"brasilia\"):[\"peru\",\"gana\",\"japão\",\"espanha\",\"india\"],\n",
    "                  (\"bahia\",\"salvador\"):[\"acre\",\"alagoas\",\"amapá\",\"amazonas\",\"ceará\",\"goiás\"],\n",
    "                  (\"brasil\",\"feijoada\"):[\"italia\",\"estados-unidos\",\"inglaterra\",\"argentina\",\"peru\"],\n",
    "                  (\"homem\",\"mulher\"):[\"garoto\",\"rei\",\"príncipe\",\"pai\",\"cavalo\",\"garçon\"],\n",
    "                  (\"grande\",\"pequeno\"):[\"cheio\",\"alto\",\"forte\",\"largo\"],\n",
    "                  (\"pelé\",\"futebol\"):[\"tyson\",\"bolt\",\"senna\"],\n",
    "                  (\"atena\",\"sabedoria\"):[\"afrodite\",\"poseidon\",\"zeus\",\"atena\"],\n",
    "                  (\"cruzeiro\",\"raposa\"):[\"atlético\",\"gremio\",\"palmeiras\",\"corinthians\"],\n",
    "                 }\n",
    "\n",
    "for (palavra,esta_para), arr_assim_como in dict_analogias.items():\n",
    "    print(f\"{palavra} está para {esta_para} assim como...\")\n",
    "    for assim_como in arr_assim_como:\n",
    "        palavras = obj_analogy.analogia(palavra,esta_para,assim_como)\n",
    "        print(f\"\\t{assim_como} está para {palavras[0]} (ou {palavras[1:]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma limitação desses embeddings é a dependencia de idioma e que palavras ambiguas não são tratadas. Por exemplo, Jaguar pode ser uma marca de carro ou animal, dependendo do contexto.  Para diminuir o problema de ambuiguidades, o [BERT](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) é um embedding que a representação da palavra é diferente de acordo com o seu contexto. O [MUSE](https://github.com/facebookresearch/MUSE) é um embedding multilingue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação textual usando embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas vezes, precisamos de um único vetor para representar uma frase ou um texto ainda maior. Para isso, podemos usar a representação Bag of Words ou, ainda, representar por palavras chaves ou utilizarmos uma combinação de nossas representações por palavras. Neste tutorial, iremos mostrar como combinar embeddings de palavras e usar a representação por palavras chaves - podendo, inclusive, fazer uma expansão de palavras chaves por embeddings.\n",
    "\n",
    "Para isso, iremos usar o seguinte contexto: por meio de um dataset de revisões de produto da amazon, deseja-se prever automaticamente o sentimento do mesmo (positivo ou negativo). Utilizou-se uma amostra do [dataset do Kaggle para este exemplo](https://www.kaggle.com/bittlingmayer/amazonreviews). Veja abaixo o dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "df_amazon_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um método de aprendizado de maquina, cada instancia deve ser representada por um vetor numérico utilizando as representações ditas anteriormente. Iremos ilustrar cada exemplo utilizando uma pequena subamostra desta amostra com 5 exemplos positivos e 5 negativos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>NIV Bible: The NIV Bible is good, but I wish I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>pouch can be better: I recently bought it at A...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>Great book!: Dawn of a Thousand Nights is extr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>Disappointed: Very small wipes canister, not v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>The most horrible Blu-Ray: First, Night scenes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "101657  NIV Bible: The NIV Bible is good, but I wish I...  positive\n",
       "49225   pouch can be better: I recently bought it at A...  positive\n",
       "158265  Great book!: Dawn of a Thousand Nights is extr...  positive\n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "200048  Disappointed: Very small wipes canister, not v...  negative\n",
       "60933   The most horrible Blu-Ray: First, Night scenes...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"positive\"][:5]\n",
    "df_negative = df_amazon_reviews[df_amazon_reviews[\"class\"]==\"negative\"][:5]\n",
    "df_amazon_mini = pd.concat([df_positive,df_negative])\n",
    "df_amazon_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words: ** um exemplo simples, sem usar embeddings, é a representação em bag of words, **já discutido aqui**. Assim, podemos  usar a classe `BagOfWords` que está no arquivo `textual_representation.py`. Para as representações bag of words, usaremos a função bag_of_words abaixo. Usando esta representação o nosso dataset ficaria representado da seguinte forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>70</th>\n",
       "      <th>absolute</th>\n",
       "      <th>actors</th>\n",
       "      <th>ahead</th>\n",
       "      <th>album</th>\n",
       "      <th>...</th>\n",
       "      <th>wooden</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>wow</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.231984</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119602</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273879</td>\n",
       "      <td>0.13694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              13        16        18        20        21        70  absolute  \\\n",
       "id                                                                             \n",
       "208138  0.115992  0.231984  0.115992  0.115992  0.115992  0.115992  0.115992   \n",
       "157010  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "101657  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "57708   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          actors     ahead     album  ...    wooden   working     world  \\\n",
       "id                                    ...                                 \n",
       "208138  0.000000  0.115992  0.115992  ...  0.000000  0.000000  0.000000   \n",
       "157010  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.239204   \n",
       "101657  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "158265  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "274316  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "57708   0.129047  0.000000  0.000000  ...  0.129047  0.000000  0.000000   \n",
       "200048  0.000000  0.000000  0.000000  ...  0.000000  0.223607  0.000000   \n",
       "60933   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "           worse     worst      wow  wrenching   written     years     class  \n",
       "id                                                                            \n",
       "208138  0.000000  0.000000  0.00000   0.115992  0.000000  0.000000  positive  \n",
       "157010  0.000000  0.000000  0.00000   0.000000  0.000000  0.119602  positive  \n",
       "101657  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "49225   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  positive  \n",
       "158265  0.000000  0.000000  0.00000   0.000000  0.245462  0.000000  positive  \n",
       "204215  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "274316  0.283463  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "57708   0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "200048  0.000000  0.000000  0.00000   0.000000  0.000000  0.000000  negative  \n",
       "60933   0.000000  0.273879  0.13694   0.000000  0.000000  0.000000  negative  \n",
       "\n",
       "[10 rows x 250 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import BagOfWords\n",
    "#o vocabulario, quando vazio, será considerado todas as palavra (menos stopwords)\n",
    "def bag_of_words(data, vocabulary=None):\n",
    "    #obtem stopwords\n",
    "    stop_words = set()\n",
    "    with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "        stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "    #instancia o bag of words, filtrando stopwords e considerando o vocabulario (se possivel)\n",
    "    bow = BagOfWords(\"bow\", stop_words=list(stop_words), words_to_consider=vocabulary)\n",
    "    \n",
    "    #o bag of words, é gerado separadamente a representação do treino e teste\n",
    "    #iremos usar apenas a representação considerando que \"data\" é o treino\n",
    "    data_preproc = bow.preprocess_train_dataset(data, \"class\")\n",
    "\n",
    "    #exibe apenas colunas não zedadas\n",
    "    m2 = (data_preproc != 0).any()\n",
    "    data_preproc = data_preproc[m2.index[m2].tolist()]\n",
    "    \n",
    "    return data_preproc\n",
    "bag_of_words(df_amazon_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words (filtrado por palavras chaves e embeddings similares)** Como bag of words é uma representação com milhares de atributos, poderiamos fazer uma restrição por palavras chaves. Por exemplo, caso usássemos como vocabulário do bag of words baseado nas palavras obtidas da roda de emoções porposta por [Scherer K., (2005)](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fury, joy, ecstatic, boredom, buoyancy, ennui, humiliating, happiness, gloom, proud, wrath, curious, worry, disgust, anxiety, indifference, apprehensive, cheer, remorse, elation, acknowledgement, contrition, aversion, denigration, contempt, temper, blame, animation, interest, relief, faith, derision, delight, exaltation, astonishing, hostile, angry, scorn, enthusiasm, hope, thunderstruck, respect, contentment, bliss, sick, alert, rage, nervous, anguish, dejected, euphoria, nausea, exhilarating, satisfaction, hopeless, resent, sadness, depreciate, happy, melancholy, infuriating, disrelish, disdain, recognition, ashamed, tear, incense, enjoy, abashed, surprise, dislike, anger, ardor, confident, guilt, abhor, shame, optimistic, mad, tedious, comfortable, chagrin, amazed, pride, sad, furious, dumbfounded, embarrassing, jittery'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "\n",
    "vocabulary = []\n",
    "for emotion_group, set_keywords in emotion_words.items():\n",
    "    vocabulary.append(emotion_group)\n",
    "    for word in set_keywords:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "\", \".join(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O grande problema é que esse grupo de palavras é muito restrito. Veja como ficou a representação dos nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interest</th>\n",
       "      <th>sick</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        interest  sick     class\n",
       "id                              \n",
       "208138       0.0   0.0  positive\n",
       "157010       1.0   0.0  positive\n",
       "101657       0.0   0.0  positive\n",
       "49225        0.0   0.0  positive\n",
       "158265       0.0   0.0  positive\n",
       "204215       0.0   0.0  negative\n",
       "274316       0.0   0.0  negative\n",
       "57708        0.0   1.0  negative\n",
       "200048       0.0   0.0  negative\n",
       "60933        0.0   0.0  negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que eliminamos as palavras que não apareceram em nenhuma instancia. Assim, como pode-se observar, apenas duas palavras foram usadas e alguns documentos não possuiam nenhuma palavra. Para ampliar o vocabulário, poderiamos expandir esta representação usando palavras similares a estas de acordo com o nosso embedding: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_expanded = []\n",
    "for word in vocabulary:\n",
    "    #obtem as 40 mais similares palavras de cada uma do vocab original\n",
    "    _,words = kdtree_embedding.get_most_similar_embedding(word,40)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja aqui as palavras usadas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"boldness, torches, tankage, open-mindedness, 're, success, culpability, jingoism, omnipresence, proclivity, wondered, misfortune, inspiration, hopeful, mocks, support, wrath, introspective, notice, bulletinyyy, always, brashness, wishes, cackles, goodness, suggesting, sensational, enjoys, provided, nonetheless, unworldly, confused, thrilling, lately, bewildered, borrowing, sandalwood, deliriously, obsession, precocity, detractors, solid, besides, apparent, mellow, forcefulness, sparks, exhilaration, disheartening, obviously, distasteful, insensitivity, crazy, instream, creations, cynicism, focus, 'm, transference, str95bb, potpourri, independency, soothed, response, seething, felicitously, imagine, regard, scare, stressing, disbelief, devotion, lambasting, satisfied, cat, calumny, shrewdness, ill-founded, evocation, bb97, unassertive, bloodlust, fun, humiliation, self-pity, retardants, equanimity, edginess, dizzying, noticed, desultory, awe, itching, hopefulness, stupidity, investor, apathy, nervousness, equal, .0202, fears, arduous, nostalgia, stupendous, treachery, apologise, aches, hipness, envious, confusion, thus, complain, prideful, visual, drubbing, patient, enchanting, fondness, intemperance, rioters, complaining, lethargy, dampened, outraged, cheerleaders, indignant, ardour, cheerfulness, morose, sorrowful, clearly, obstinate, hypnotised, raucous, broken, misery, panache, expression, thrill, surely, covetousness, dispiriting, noting, heartfelt, stones, discouragement, maddening, piety, montanans, confidence, candles, over-produced, loneliness, angering, liking, contrition, delighted, liberality, thank, earnestness, cathartic, insisted, uninitiated, unexcited, purposefulness, reassuring, egomania, furor, suffering, befuddling, blame, crispness, thing, nastiness, calling, displeasing, loathing, anti-clericalism, dullness, ironic, exaltation, warned, perfection, divine, shameful, greet, fretful, exhausting, cushioned, messy, giddiness, unambiguous, inscrutably, hilarity, tellingly, overanalyze, prefer, flame, staying, anticipating, distrusted, kid, recognize, steadiness, nonplussed, perspire, passivity, alarmed, opposed, calamitous, surprisingly, achievement, crunch, unsettling, appreciation, continue, see, pleasurably, falseness, signal, symbolized, wary, rather, dubbed, burn, realize, puzzlement, complacency, vwahr, wonderment, agony, worried, bullish, loving, breathtaking, depredations, mendacity, amusing, reflecting, recognised, peacefulness, unfriendly, regretful, cry, immediately, disregard, action-packed, confident, nuttiness, hesitancy, ambition, hesitant, wearying, films, undecipherable, dumbfounded, circumspect, irritation, firecrackers, earthiness, interactive, rehabilitation, orderliness, idiocy, claiming, belligerent, kd97, misunderstood, grenades, reproach, passion, gentility, strongly, spite, hopes, principle, exuberance, afraid, monotony, love, liberty, serenity, nowhere, frustrating, thermoregulation, visibly, assure, blunt, resisting, explode, payback, unseemly, indulged, displeased, dignity, distrust, suspenseful, attacking, southpaws, despite, sassacus, animation, scary, k587-1, headache, impatient, exploding, acetylene, treating, encumber, riveting, animations, otherwise, purifier, hostile, overconfident, groggy, resented, enthusiasm, hope, inconsolable, signified, attentiveness, unimpressed, aid, antagonistic, insanity, canonicus, rage, offering, spirit, plenty, nausea, expectations, solitude, constrain, heaped, acasta, benefit, sentimental, lament, childlike, silliness, increasing, bruising, vulnerability, useless, upon, way, neuroses, glory, demoralising, looking, astounding, pointless, worship, prosecution, devaluating, enjoy, intense, vigilance, innocence, dispersed, newborn, anger, ardor, deplore, reassure, accepted, true, paradoxically, concerns, dismay, riot, comfortable, judgment, sad, regret, jaded, gripped, spray, anguished, irked, depreciating, predicting, idolize, disorienting, sadly, hardly, ought, dazzling, re-visited, awkwardness, disney, inexplicable, demeaning, remarkable, lanterns, mortified, propensity, tearing, christian, fixated, indifferent, greg.wilcoxdailynews.com, easy, burning, accomplished, .0342, shout, incomparable, remorse, elation, despises, outspokenness, getting, welcome, crowds, recognizing, contempt, weekend, advocation, exoneration, irate, intemperate, photography, srivalo, asserting, affirmation, dying, ire, uselessness, penury, mystified, helping, pitiful, angst, believes, here, munificence, opprobrium, abhorred, complacent, incredible, cheerily, plausibility, rw95, uncouth, diarrhoea, illness, interestingly, everyone, anguish, acknowledgment, slovenliness, x.xx.xx.xx.x, spend, distraught, revenge, care, amazing, unamused, rejoicing, demented, fluidity, absurdity, competence, infuriated, yearn, .0170, suzuya, farcical, sympathy, reluctance, rheology, graciousness, applaud, relaxed, throngs, flummoxed, happy, smugness, maelstrom, ungenerous, cramping, torturous, fend, pranked, price, recognizes, thankful, failure, celebrating, lechery, trouble, decisiveness, mesmerizing, laughter, beeswax, predominance, cgi, misunderstand, unbothered, future, revelations, fiery, cramps, evident, liken, wistful, biotechtrst, depletes, hatter, pride, termed, inevitable, affronted, arguing, contend, feistiness, jealousy, condescension, pervading, adore, censers, triumphalism, repetitious, dislikes, lucky, evoked, giving, mournful, fortunate, propriety, overdressed, rattled, unsatisfied, feel, delightful, believe, insufferable, surreal, diffident, grateful, .0207, proud, ignored, unpleasant, burned, remember, cruel, demonization, spellbinding, sickness, disgust, believer, contingent, quietness, revelatory, ornery, shamefaced, evildoers, bb96, .0163, luck, babies, epically, dizziness, achieved, melancholic, wacky, saw, mocked, occasion, disperse, tantrum, angered, laborious, ponderous, bombast, madness, throwing, downsize, repudiate, puzzling, internalize, addition, folks, miserable, poignancy, friendliness, pensive, spraying, offended, raising, provoked, bring, affected, soulful, dissapointed, vituperation, http://www.opel.com, haunting, hauteur, peevish, looks, nervous, fabulousness, honored, invective, enjoying, cow, loath, reminded, why, profess, cue, licentiousness, provide, baffling, unrelenting, similarly, shaken, endearing, downcast, nonplused, vengeful, enraging, despise, emphasized, preoccupied, curiously, turning, guiltily, skeptical, want, viciousness, thrilled, stoicism, gyroscopic, troubling, zeal, definitely, dejection, scripts, burners, motivation, catharsis, predicament, weariness, ballast, abhor, edgy, quietude, unsurprisingly, whimsy, cannons, migraines, afflicted, followed, doomed, chore, embarrassment, interpenetration, propitiation, ineptitude, callousness, seeing, surprises, goofiness, stunned, bad, appalled, much, disliked, gratitude, shrilly, suited, penchant, unconvinced, dog, garlands, sitting, warn, pains, physicality, exasperating, realizing, libations, everybody, cheer, merriment, reflexively, think, euoplocephalus, suggests, timidity, brominated, exclusivism, sort, signifying, truth, glib, need, dreamworks, helplessness, feature, christ, blurbed, distressing, stabiliser, str94, anticipate, david.lazarus@latimes.com, weary, shocked, needs, odd, predilection, upset, suggestion, nerves, deployed, insulted, sprayed, diminishes, goal_montreal, detestation, engaging, engendered, sick, bereft, encouragement, inviting, disrespected, nevertheless, inclusion, youthfulness, disgraceful, defeats, forgiveness, cowardliness, disconsolate, timeless, horrendous, protectiveness, eagerness, unsatisfying, resent, belated, legitimacy, melancholy, infuriating, reflected, mayhem, peeved, k978-1, intimation, certainly, disgusted, mind, furthermore, shouts, 28aou94, eschew, nor, maneuverability, indication, uninteresting, ironically, importance, depreciated, fervor, ready, triumph, magnificent, manner, dreadful, although, pleasure, startled, concerned, evacuation, twitchy, manoeuvre, jubilation, rp-1, bewilderment, depreciates, oblivious, academicism, firestorm, coarseness, shortness, honor, experience, restlessness, cleverness, stress, unleashed, inattention, insistent, reassurances, artfulness, worry, overjoyed, bitterness, hostility, dispatched, pleased, aimlessness, inflexibility, dismayed, heft, loathe, letting, imprimatur, wearisome, terrible, jumpy, circularity, jubilant, cuteness, amidst, vivacity, wicked, tantrums, interest, shocking, wariness, creative, incredulity, dread, constricting, mercy, turn, infuriates, overact, celebrate, topside, clumsiness, stupid, acceptance, undeterred, impress, guileless, admirable, evocative, rousing, stylish, stupefied, wisdom, fascination, overwrought, timelessness, regrets, discomfiting, headaches, aback, jolting, clueless, frenzy, selflessness, em96, impotent, darkening, fearing, hell, attitude, perceived, protection, recognized, frenzied, chance, watching, romanticize, comfortably, scapegoating, depreciate, essence, tears, firebombs, sake, heartburn, abrade, cannon, yet, jitters, lg03, flabbergasted, ridicule, reconsideration, wrong, ignorance, bringing, fulfillment, sociability, omission, mad, frustrations, despondency, terrifying, unhappy, pathos, denunciation, dissatisfaction, threat, proclivities, dramatic, ruminative, superlatives, prompting, crestfallen, chafe, wake, disconcerted, ridiculousness, likely, creativity, speculation, astounded, glad, studios, rescue, humiliating, ugly, happiness, cheers, peculiar, expecting, outbursts, hydrodynamic, unhappiness, callous, stubbornness, indifference, unconcerned, unquestionable, treated, fearlessness, unforgettable, reverence, script, aversion, mortifying, sticks, convinced, staggering, fretted, annoyance, survivability, look, providing, js04bb, selfishness, emasculation, flustered, disinterest, votive, defiance, pessimism, homesickness, formulaic, confess, anticipation, horrified, exoticism, tihg, thunderstruck, kalamity, demonstrate, depressed, chagrined, mockery, pity, productions, claustrophobia, impression, bemoan, demonstrators, brutalization, frustrated, guffaws, unfathomable, esteem, worse, .000105, exclamations, poignant, retribution, fantastic, frightening, musculature, .000088, liveliness, dispersing, languor, tragic, infused, explicitness, ignominious, summons, ferocity, hoped, recognition, dispirited, participation, vilification, bullishness, irritated, wanting, aggressive, simply, exasperation, brought, disgrace, inadequacy, awesome, pellets, howls, despair, crazed, stupefaction, antigravity, rejecting, generalise, irreverence, 'll, discomfort, magnanimity, latest, prospects, anticipated, desire, anxieties, ennui, compassion, bemusement, sanctimony, malevolence, engrossing, tumult, despondent, thoughtlessness, seeming, displeasure, camphor, petulance, assistance, coldness, 65stk, homeless, sloppiness, apprehensive, http://www.oklahomacitynationalmemorial.org, filmmakers, talky, understand, foreboding, effort, shock, hypocritical, crowd, strange, integrity, accustomed, illumined, precaution, status, acquiescence, thanks, awkward, angry, concur, perfect, visualise, sublime, apologize, hopelessness, conscience, compensator, ripoffs, threatening, shouting, belief, disillusionment, leniency, quite, perceive, else, temperament, discouraged, pretty, panicked, come, tempted, disgruntled, ordeal, hubbub, disillusion, bizarre, acknowledging, constricted, symbolize, animator, alarm, moreover, questioning, parents, though, exasperated, upsetting, sadness, invoking, saves_mrivera, came, painstaking, 3-d, calmed, engendering, idleness, wo, undeniable, abashed, ambivalent, shyness, tiring, appreciate, bullet, gleeful, lit, truly, understanding, hug, indignation, mothers, shame, fahnt, bemused, comforted, unleashes, didacticism, action, joyful, impressed, prompts, lust, fear, reason, unease, geotagging, villainy, deflate, rectitude, fury, ill-informed, enjoyable, miffed, bullets, blaming, gobsmacked, equate, prosecuted, emergency, yearning, fading, legacy, canisters, perplexed, ethos, storyboard, conviction, loyalty, lopsided, resents, wondering, insinuation, denunciations, scorned, doubt, mesmerized, perfunctory, sparklers, excited, anxiety, uneasiness, delirious, frankincense, js94bb, looked, kd96, discombobulated, denigration, putting, fxff, disappointment, admiring, given, predictably, distracting, stay, rebukes, preoccupation, hungry, significance, loved, ambuscade, exhortation, applauding, jaundice, unconvincing, surveillance, eloquence, underwhelmed, tradition, scorn, skillz, feelings, contentment, remembered, promise, concision, cautiousness, implication, coded, speechless, puzzled, reticence, insomnia, vengeance, drudgery, storytelling, antipathy, malaise, euphoria, frazzled, nasty, hysteria, naborsind, exhilarating, realization, dreams, affection, mindful, satisfaction, unfortunate, decent, appreciating, certain, lyricism, disaster, embrace, remorseful, fergalicious, find, indigestion, seductive, adulation, scared, joke, consternation, supplant, interesting, sent, politeness, respects, unbelievable, reverie, rates, monster, denouncement, glum, suspicion, uninterested, cautioned, tiredness, grace, shelter, cautiously, migraine, going, directness, engaged, foolish, panicky, unironic, bittersweet, grief, lending, embarrassing, protesters, compliments, fatigued, bdb94, inquisitive, untruth, rootlessness, enraged, kudos, emotionalism, swoon, assurance, paranoia, implying, celebration, aghast, approbation, moral, peppermints, curious, starving, shadows, mo95, trivialize, cushioning, affectations, exuberant, pixar, piyanart, intrigued, sudden, unexpected, 30-270, voluptuousness, better, longevity, sensitivity, assist, enigmatically, sweating, marveled, impressive, http://www.nifc.gov/, sympathetic, arrogance, efforts, clamor, derision, ghastly, cries, stressed, engagement, astonishing, disgusting, help, achievements, passions, palpitations, appreciative, establishment, captivating, tendency, testifying, entranced, sending, disenchantment, townsfolk, 24aou94, fevers, admonition, livid, patronize, scandalous, visuals, self-love, live-action, reprisal, heightened, unsure, lonely, menorah, irony, irrepressible, outcry, heartwarming, hurled, overabundance, leave, distracted, overconfidence, doubles_biggio, homoeroticism, fatalism, value, rashes, know, batons, deprogrammed, disastrous, resultant, js03, discontent, raised, incense, studio, crankiness, devaluing, concern, overlong, minions, freedom, stunning, uniqueness, retaliation, resentment, merciless, importantly, abandon, optimism, optimistic, current, brave, newfound, hurry, bhagya, cacophony, say, cumbersome, apology, intimidated, coloradans, sense, de-emphasize, dreamy, playful, disheartened, impatience, animated, antsy, boredom, regards, kd94, buoyancy, carnality, solipsism, insolence, pretentiousness, unleash, amorality, gloom, brightly, dishonesty, engage, palpable, apologizing, multimedia, emotions, lifeline, protesting, insipid, immediate, coming, unnerving, greatness, good, incredulous, responding, spurred, unfazed, curiosity, rebuilding, skepticism, forlorn, enjoyed, reticent, grumble, pronouncement, sympathize, rebelliousness, blandness, strangeness, betrayed, unfairness, depressing, assured, admire, warrant, constipation, forsake, regardless, pessimistic, frustration, pleasantness, sinicization, convoluted, desperation, k977-1, fact, film, exactitude, fit, bliss, frantic, recovery, frankly, hysterical, symptoms, contribution, thankless, counter, god, playfulness, indeed, splendid, referring, grotesque, hopeless, unflustered, weird, counteract, awful, unfortunately, dehydration, …, astonishment, uncomfortable, castigation, hoping, degrading, surliness, time-consuming, dislike, slander, devaluate, unprepared, compounded, ornaments, higher, allied, grandiosity, heartbroken, tiresome, injustice, aggressiveness, excellence, rise, indestructibility, destitution, knowing, opposing, keeping, alerting, nice, solemnity, take, cowardice, viewed, oly-2004-fhockey, boisterous, horrific, presumption, unsettled, defensiveness, sprinkled, shellshocked, grossness, mania, put, biodegrade, buying, malice, rest, eventful, phenomenal, ethereal, humility, spared, amused, idiosyncrasy, mistake, equated, unserious, threats, supportive, agitate, clear, alarms, perturbed, fool, mtow, buoyant, benignly, _____________________________________________, hypocrisy, erratic, bb94, tranquillity, apotheosis, despairing, pleasantly, 'd, privilege, hand-drawn, wondrous, uncertainty, uplifting, unfocused, intellectualism, neediness, temper, mystifying, presume, panic, debase, deepens, faith, www.slarmy.org, clarity, unprompted, searing, trepidation, clumsy, .0208, crystallise, irritability, insist, powerless, animators, reconstruction, helpless, greenness, permanence, vruhl, curses, expressing, moralize, respect, puppetry, enamored, religion, kindling, nowadays, allusion, distaste, censer, committment, behest, beast, dehumanization, electrifying, dejected, teargas, sorry, bangkokians, washingtonians, expectation, relentless, flamboyance, fervour, stop-motion, soothing, blames, complained, demonstrates, humiliated, sullen, shamed, admired, mistaken, healthy, sensuous, humbling, ashamed, envy, cautious, alerts, reassured, shields, surprising, deform, nonchalance, embittered, booing, beenz, haughtiness, cheered, wonderful, gph04bb, rudeness, tempered, decry, amazed, feeling, obsessed, editing, simple-minded, keep, insult, irksome, particular, unlikely, contrarians, protestors, annoyed, clamoring, fascinating, error-prone, ignore, frivolity, joy, humanitarian, perfidy, blamed, meant, thoughtfulness, fearful, hesitation, fascinated, enjoyment, disorientation, familiarity, informality, fatuous, lest, takeover, frightened, ingratitude, copal, affectation, despised, euphoric, emotion, depth-charged, exultant, fatigue, effervescence, immortality, danger, admiration, dumbstruck, self-monitoring, outrage, admitting, ignoring, pig, fired, rollicking, relief, warnings, detest, raisonnable, vatten, merits, flood, commitment, individualization, pregnant, dismaying, craving, treat, alerted, maybe, embarassing, aware, uproar, thrown, compelled, surprised, debasement, distinction, awed, patriotism, pathetic, candor, fierce, destitute, excercise, risible, longing, wonder, rabbit, elderly, excruciatingly, cheering, uncertain, airiness, enlightening, irritating, candle, disturbing, devalue, terrified, myrrh, inscrutable, agitated, horrible, antagonism, catcalls, elaboration, disliking, unremorseful, disdain, recognising, heartbreaking, flatten, documentary, torment, virtue, squeamish, ambivalence, inability, marvellous, cartoon, doldrums, amazement, appalling, gloomy, postmodernists, withering, frankness, malevolent, tedious, intimacy, furious, determined, messiness, inconstancy, needed, redemption, invigorating, avoid, ..., sluggishness, rallying, joyless, resenting, soon, awestruck, vega@globe.com, believing, depress, wanderlust, uncharacteristic, impishly, marvelous, snobbery, pictures, anxious, enthused, benumbed, omniscience, likening, oddness, vulnerable, delights, wafted, whatever, wretched, humaneness, explain, unnerved, oftentimes, hatred, expect, delight, sanguine, re-organise, votives, baffled, canister, self-identity, stun, qualms, heartache, publicized, doctrine, determination, ingenuous, adventurousness, embarrassed, greeted, understandably, dishonour, genuine, spirituality, alert, explosiveness, ferocious, wanted, heartlessness, drohs, skittish, diarrhea, ominous, http://www.mediabynumbers.com, moment, nightmare, skylarking, masterful, lucidity, fluctuate, incensed, caring, sight, excitement, plaudits, beliefs, insouciance, combativeness, elated, believers, ill, continuing, organgn, incredibly, bravado, surprise, forget, stranger, pleading, filmmaking, drowsiness, actions, brooding, opportunity, lightheadedness, outsiders, quick, chagrin, comfy, mims, prompted, horrifying, compensators, backlash, astonished, theatrical, popi, immaturity, jittery, discomfited, betrayal, abasement, patterson, gentleness, contrary, scornful, ecstatic, soothe, meekness, perfectionism, boring, mishandle, check, vitriol, restless, plodding, menace, resentful, firing, kroyts, laziness, tedium, bafflement, radiance, denial, lobbing, elicited, distraction, imperieuse, worrying, wishing, acknowledgement, rubber, symbolizes, ascribe, disappointed, sorrowfully, insularity, courtesy, accuse, savage, video, inconsequential, admit, give, idealize, empathy, really, joyous, warlike, onslaught, demoralizing, respecting, .0206, tired, gone, humble, pain, perplexity, heartless, acknowledge, monotonous, soulfulness, menacing, spooked, anyway, sleeplessness, burst, elegiac, doom, sure, alabamians, brokenhearted, upbeat, individuality, perturb, warning, praise, cinematic, scented, sentimentality, reaffirmation, bedraggled, detests, transcendence, eager, expressed, beguiling, apologies, precautions, sorrow, bloating, spirited, startling, tear, vomiting, skyrocket, alienation, cared, vigilant, enthralling, announcement, usual, swings, neither, antics, theatricality, befuddled, lollipops, guilt, repentance, loveliness, breathless, oly-2004-tennis, stirred, disrespect, nosebleeds, raise, debauchery, revulsion, confronted, giddy, wish, leery, entrancing, recklessness, exhilarated\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas palavras podem não estar relacionadas à emoção, porém, o método de aprendizado de máquina ainda é capaz de considerar palavras mais relevantes para uma determinada instancia, ignorando algum ruído. Veja como ficou a representação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>canister</th>\n",
       "      <th>disappointed</th>\n",
       "      <th>experience</th>\n",
       "      <th>fact</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>find</th>\n",
       "      <th>give</th>\n",
       "      <th>god</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>put</th>\n",
       "      <th>putting</th>\n",
       "      <th>sense</th>\n",
       "      <th>sick</th>\n",
       "      <th>sympathetic</th>\n",
       "      <th>true</th>\n",
       "      <th>video</th>\n",
       "      <th>wanting</th>\n",
       "      <th>worse</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.490297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.606043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606043</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.474727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             bad  canister  disappointed  experience      fact  fascinating  \\\n",
       "id                                                                            \n",
       "208138  0.000000  0.000000      0.000000    0.606043  0.000000     0.000000   \n",
       "157010  0.000000  0.000000      0.000000    0.000000  0.490297     0.490297   \n",
       "101657  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "49225   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "158265  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "204215  0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "274316  0.606043  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "57708   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "200048  0.000000  0.707107      0.707107    0.000000  0.000000     0.000000   \n",
       "60933   0.000000  0.000000      0.000000    0.000000  0.000000     0.000000   \n",
       "\n",
       "        find      give  god      good  ...       put   putting     sense  \\\n",
       "id                                     ...                                 \n",
       "208138   0.0  0.606043  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "157010   0.0  0.000000  0.0  0.324199  ...  0.416798  0.000000  0.000000   \n",
       "101657   0.0  0.000000  0.0  1.000000  ...  0.000000  0.000000  0.000000   \n",
       "49225    0.0  0.000000  0.0  0.551556  ...  0.000000  0.000000  0.000000   \n",
       "158265   0.0  0.000000  0.5  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "204215   0.0  0.000000  0.0  0.000000  ...  0.515192  0.000000  0.606043   \n",
       "274316   0.0  0.000000  0.0  0.000000  ...  0.000000  0.515192  0.000000   \n",
       "57708    0.5  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "200048   0.0  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "60933    0.0  0.000000  0.0  0.313903  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        sick  sympathetic      true     video  wanting     worse     class  \n",
       "id                                                                          \n",
       "208138   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "157010   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "101657   0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "49225    0.0          0.0  0.000000  0.000000      0.0  0.000000  positive  \n",
       "158265   0.0          0.0  0.000000  0.000000      0.5  0.000000  positive  \n",
       "204215   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "274316   0.0          0.0  0.000000  0.000000      0.0  0.606043  negative  \n",
       "57708    0.5          0.5  0.000000  0.000000      0.0  0.000000  negative  \n",
       "200048   0.0          0.0  0.000000  0.000000      0.0  0.000000  negative  \n",
       "60933    0.0          0.0  0.474727  0.474727      0.0  0.000000  negative  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words(df_amazon_mini,vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderiamos agrupar as palavras chaves em conceitos, por exemplo, \"happiness\" ser sempre contabilizado quando houver um conjunto de palavras, por exemplo, '\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"'. Porém, isso pode restringir muito o número de palavras e expandir com palavras usando embeddings, pode extrair palavras relacionadas com a emoção oposta (veja exemplo abaixo). Por isso, optamos por apresentar a representação usando bag of words. Mesmo assim, caso queira ver algum resultado dessa forma, a classe CountWords implementa expansão por grupos de palavras chaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"happy, feel, glad, sure, everyone, 'm, definitely, 'd, 'll, remember, everybody, wish, proud, 're, really, always, maybe, excited, good, lucky, obviously, thrilled, pleased, pretty, wonderful, know, afraid, delighted, looking, want, thing, imagine, think, unhappy, satisfied, realize, knowing, going, tired, crazy\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distance, words = kdtree_embedding.get_most_similar_embedding(\"happy\",40)\n",
    "#veja que unhappy é relacionado com happy - além de outras palavras negativas e ruido\n",
    "\", \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pride</th>\n",
       "      <th>elation</th>\n",
       "      <th>happiness</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>relief</th>\n",
       "      <th>hope</th>\n",
       "      <th>interest</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>sadness</th>\n",
       "      <th>boredom</th>\n",
       "      <th>shame</th>\n",
       "      <th>guilt</th>\n",
       "      <th>disgust</th>\n",
       "      <th>contempt</th>\n",
       "      <th>hostile</th>\n",
       "      <th>anger</th>\n",
       "      <th>recognition</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pride  elation  happiness  satisfaction  relief  hope  interest  \\\n",
       "208138      0        0         14             0       0     7         0   \n",
       "157010      0        0          5             0       0     5         2   \n",
       "101657      0        0          7             0       0     5         0   \n",
       "49225       0        0          4             0       0     3         0   \n",
       "158265      0        0          3             0       0     1         0   \n",
       "204215      0        0          0             0       0     1         0   \n",
       "274316      0        0          2             0       0     0         0   \n",
       "57708       0        0         10             0       0     7         0   \n",
       "200048      0        0          4             0       0     2         0   \n",
       "60933       0        0         12             0       0     2         0   \n",
       "\n",
       "        surprise  anxiety  sadness  boredom  shame  guilt  disgust  contempt  \\\n",
       "208138         0        4        0        0      0      0        0         0   \n",
       "157010         0        3        0        0      0      0        0         0   \n",
       "101657         0        0        0        0      0      0        0         0   \n",
       "49225          0        1        0        0      0      0        0         0   \n",
       "158265         0        1        0        0      0      0        0         0   \n",
       "204215         0        1        0        0      0      0        0         0   \n",
       "274316         0        0        0        0      0      0        0         0   \n",
       "57708          0        5        0        0      0      0        1         0   \n",
       "200048         0        0        0        0      0      0        0         0   \n",
       "60933          0       12        1        0      0      0        0         0   \n",
       "\n",
       "        hostile  anger  recognition     class  \n",
       "208138        0      0            0  positive  \n",
       "157010        0      0            0  positive  \n",
       "101657        0      0            1  positive  \n",
       "49225         0      0            0  positive  \n",
       "158265        0      0            0  positive  \n",
       "204215        0      0            0  negative  \n",
       "274316        0      0            0  negative  \n",
       "57708         0      0            0  negative  \n",
       "200048        0      0            0  negative  \n",
       "60933         0      0            0  negative  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.textual_representation import CountWords,InstanceWisePreprocess\n",
    "aggregate = CountWords(dict_embedding, emotion_words,max_distance=0.3)\n",
    "\n",
    "word_counter = InstanceWisePreprocess(\"word-counter\",aggregate)\n",
    "word_counter.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O max_distance é resposável por obter as palavras similares. Veja que diversos documentos negativos foram classficados com o grupo \"happiness\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representação agregando embeddings das palavras: ** Conforme proposto por [Shen et al.](https://arxiv.org/pdf/1805.09843.pdf), dado que uma frase é representado por um conjunto de embeddings $\\{e_1, e_2, ..., e_n\\}$  uma forma simples e que geralmente obtém resultados **comparáveis a métodos mais complexos** é fazer operações em cada dimensão do embedding, tais como: média e máximo por dimensão do embedding. Por exemplo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'house', 'is', 'green']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings de alguams palavras: \n",
    "dict_embedding = {'my':      [10, 11,14, 20, 15, 80],\n",
    "                  'house':   [11, 12,10, 24, 11, 30],\n",
    "                  'is':      [1,  3,  5, -1, 10, 20],\n",
    "                  'green':   [12,10, 20, 12, 10, 20]\n",
    "                   }\n",
    "#representação do texto \"my house is green\"\n",
    "arr_texto = \"my house is green\".split()\n",
    "arr_texto      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando a média de cada dimensão dos embeddings:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [8.5, 9.0, 12.25, 13.75, 11.5, 37.5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula a média da iésima posição do embedding\n",
    "        sum_pos = 0\n",
    "        for word in arr_texto:\n",
    "            sum_pos += dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(sum_pos/len(arr_texto))\n",
    "    return representacao\n",
    "dim_embedding = 6\n",
    "representacao = average_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando o máximo de cada dimensão dos embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representação: [12, 12, 20, 24, 15, 80]\n"
     ]
    }
   ],
   "source": [
    "dim_embedding = 6\n",
    "def max_pooling(arr_texto, dim_embedding):\n",
    "    representacao = []\n",
    "    for i in range(dim_embedding):\n",
    "        #calcula o valor máximo de cada iésima posição do embedding\n",
    "        first_word = arr_texto[0]\n",
    "        max_pos = dict_embedding[first_word][i]\n",
    "        for word in arr_texto[1:]:\n",
    "            if max_pos < dict_embedding[word][i]:\n",
    "                max_pos = dict_embedding[word][i]\n",
    "\n",
    "        representacao.append(max_pos)\n",
    "    return representacao\n",
    "representacao = max_pooling(arr_texto, dim_embedding)\n",
    "print(f\"Representação: {representacao}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como há palavras pouco relevantes (como stopwords) podemos remove-las e, também podemos utilizar apenas as palavras de um vocabulario controlado. Abaixo veja a representação. Como esta representação é vetorial, a mesma não é uma representação simples de ser entendida por humanos, porém, pode-se obter bons resultados. Você pode adicionar o vocabulario controlado ou as stopwords por meio dos parametros correpondentes. O parametro `aggregate_method` define se será feito um maximo ou média entre os embeddings colocando os valores `max` ou `avg`, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\steff\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.371582</td>\n",
       "      <td>0.357178</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>-0.347168</td>\n",
       "      <td>-0.115784</td>\n",
       "      <td>-0.361572</td>\n",
       "      <td>-0.089905</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>-0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254395</td>\n",
       "      <td>0.132202</td>\n",
       "      <td>-0.341553</td>\n",
       "      <td>-0.369385</td>\n",
       "      <td>-0.165894</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.372070</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>-0.066101</td>\n",
       "      <td>0.366699</td>\n",
       "      <td>0.482666</td>\n",
       "      <td>-0.193726</td>\n",
       "      <td>-0.203979</td>\n",
       "      <td>-0.025436</td>\n",
       "      <td>-0.202759</td>\n",
       "      <td>-0.129517</td>\n",
       "      <td>-0.123230</td>\n",
       "      <td>-0.393066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131592</td>\n",
       "      <td>0.095581</td>\n",
       "      <td>-0.328857</td>\n",
       "      <td>-0.464355</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>-0.303223</td>\n",
       "      <td>-0.488037</td>\n",
       "      <td>0.161743</td>\n",
       "      <td>0.561035</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>-0.108276</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>0.379395</td>\n",
       "      <td>-0.305664</td>\n",
       "      <td>-0.473633</td>\n",
       "      <td>-0.043793</td>\n",
       "      <td>-0.298096</td>\n",
       "      <td>-0.203247</td>\n",
       "      <td>-0.172852</td>\n",
       "      <td>-0.349609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209839</td>\n",
       "      <td>0.254150</td>\n",
       "      <td>-0.035828</td>\n",
       "      <td>-0.753906</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>-0.158447</td>\n",
       "      <td>-0.395020</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.669922</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49225</th>\n",
       "      <td>-0.101196</td>\n",
       "      <td>0.435791</td>\n",
       "      <td>0.408936</td>\n",
       "      <td>-0.194336</td>\n",
       "      <td>-0.423340</td>\n",
       "      <td>-0.323730</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>-0.204224</td>\n",
       "      <td>-0.406006</td>\n",
       "      <td>-0.180054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295410</td>\n",
       "      <td>0.097839</td>\n",
       "      <td>-0.178955</td>\n",
       "      <td>-0.645508</td>\n",
       "      <td>0.090698</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>-0.361816</td>\n",
       "      <td>0.333740</td>\n",
       "      <td>0.679688</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158265</th>\n",
       "      <td>0.449951</td>\n",
       "      <td>0.435547</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>-0.087097</td>\n",
       "      <td>-0.048950</td>\n",
       "      <td>0.714844</td>\n",
       "      <td>-0.627441</td>\n",
       "      <td>-0.139526</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>-0.392822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>0.201050</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.448730</td>\n",
       "      <td>-0.454346</td>\n",
       "      <td>-0.360840</td>\n",
       "      <td>-0.517578</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>-0.052979</td>\n",
       "      <td>0.248535</td>\n",
       "      <td>0.510742</td>\n",
       "      <td>-0.238281</td>\n",
       "      <td>-0.316162</td>\n",
       "      <td>0.045197</td>\n",
       "      <td>-0.314941</td>\n",
       "      <td>-0.398926</td>\n",
       "      <td>-0.188843</td>\n",
       "      <td>-0.045197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223267</td>\n",
       "      <td>0.028717</td>\n",
       "      <td>-0.146606</td>\n",
       "      <td>-0.456055</td>\n",
       "      <td>-0.174316</td>\n",
       "      <td>-0.062744</td>\n",
       "      <td>-0.412842</td>\n",
       "      <td>0.221436</td>\n",
       "      <td>0.521973</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>0.158936</td>\n",
       "      <td>-0.016418</td>\n",
       "      <td>0.793945</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.669434</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>-0.129272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127441</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.206665</td>\n",
       "      <td>-0.333496</td>\n",
       "      <td>0.056488</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.239014</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>0.103943</td>\n",
       "      <td>0.467529</td>\n",
       "      <td>0.342285</td>\n",
       "      <td>-0.302979</td>\n",
       "      <td>-0.459717</td>\n",
       "      <td>0.056305</td>\n",
       "      <td>-0.398682</td>\n",
       "      <td>-0.035187</td>\n",
       "      <td>0.303223</td>\n",
       "      <td>-0.378906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196655</td>\n",
       "      <td>0.225342</td>\n",
       "      <td>-0.259521</td>\n",
       "      <td>-0.640625</td>\n",
       "      <td>-0.073242</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>-0.088257</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>0.272949</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200048</th>\n",
       "      <td>-0.183350</td>\n",
       "      <td>0.038147</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>-0.322754</td>\n",
       "      <td>-0.443848</td>\n",
       "      <td>0.263184</td>\n",
       "      <td>-0.166382</td>\n",
       "      <td>0.372803</td>\n",
       "      <td>0.187622</td>\n",
       "      <td>-0.003580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205322</td>\n",
       "      <td>-0.045746</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.231567</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>0.063171</td>\n",
       "      <td>0.338623</td>\n",
       "      <td>-0.086243</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60933</th>\n",
       "      <td>-0.002674</td>\n",
       "      <td>0.322754</td>\n",
       "      <td>0.522949</td>\n",
       "      <td>-0.489014</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>-0.020935</td>\n",
       "      <td>0.125732</td>\n",
       "      <td>-0.225220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125610</td>\n",
       "      <td>-0.003860</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.171265</td>\n",
       "      <td>-0.366943</td>\n",
       "      <td>-0.346680</td>\n",
       "      <td>0.191772</td>\n",
       "      <td>0.468506</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "208138  0.003941  0.371582  0.357178  0.001246 -0.347168 -0.115784 -0.361572   \n",
       "157010 -0.066101  0.366699  0.482666 -0.193726 -0.203979 -0.025436 -0.202759   \n",
       "101657 -0.108276  0.228638  0.379395 -0.305664 -0.473633 -0.043793 -0.298096   \n",
       "49225  -0.101196  0.435791  0.408936 -0.194336 -0.423340 -0.323730 -0.024170   \n",
       "158265  0.449951  0.435547  0.288086 -0.087097 -0.048950  0.714844 -0.627441   \n",
       "204215 -0.052979  0.248535  0.510742 -0.238281 -0.316162  0.045197 -0.314941   \n",
       "274316  0.158936 -0.016418  0.793945 -0.266113 -0.669434 -0.004871 -0.398193   \n",
       "57708   0.103943  0.467529  0.342285 -0.302979 -0.459717  0.056305 -0.398682   \n",
       "200048 -0.183350  0.038147  0.314453 -0.322754 -0.443848  0.263184 -0.166382   \n",
       "60933  -0.002674  0.322754  0.522949 -0.489014 -0.570312 -0.012024 -0.171021   \n",
       "\n",
       "               7         8         9  ...        91        92        93  \\\n",
       "208138 -0.089905  0.030563 -0.239990  ... -0.254395  0.132202 -0.341553   \n",
       "157010 -0.129517 -0.123230 -0.393066  ... -0.131592  0.095581 -0.328857   \n",
       "101657 -0.203247 -0.172852 -0.349609  ... -0.209839  0.254150 -0.035828   \n",
       "49225  -0.204224 -0.406006 -0.180054  ... -0.295410  0.097839 -0.178955   \n",
       "158265 -0.139526  0.037781 -0.392822  ... -0.330322  0.201050  0.036438   \n",
       "204215 -0.398926 -0.188843 -0.045197  ... -0.223267  0.028717 -0.146606   \n",
       "274316 -0.331055  0.040588 -0.129272  ... -0.127441 -0.073242 -0.206665   \n",
       "57708  -0.035187  0.303223 -0.378906  ... -0.196655  0.225342 -0.259521   \n",
       "200048  0.372803  0.187622 -0.003580  ... -0.205322 -0.045746 -0.363281   \n",
       "60933  -0.020935  0.125732 -0.225220  ... -0.125610 -0.003860 -0.136719   \n",
       "\n",
       "              94        95        96        97        98        99     class  \n",
       "208138 -0.369385 -0.165894 -0.306641 -0.372070  0.321533  0.433105  positive  \n",
       "157010 -0.464355  0.001116 -0.303223 -0.488037  0.161743  0.561035  positive  \n",
       "101657 -0.753906  0.020142 -0.158447 -0.395020 -0.072266  0.669922  positive  \n",
       "49225  -0.645508  0.090698  0.038300 -0.361816  0.333740  0.679688  positive  \n",
       "158265 -0.448730 -0.454346 -0.360840 -0.517578  0.019989  0.399902  positive  \n",
       "204215 -0.456055 -0.174316 -0.062744 -0.412842  0.221436  0.521973  negative  \n",
       "274316 -0.333496  0.056488  0.071777  0.239014  0.317383 -0.004375  negative  \n",
       "57708  -0.640625 -0.073242 -0.130859 -0.088257  0.115723  0.272949  negative  \n",
       "200048 -0.231567  0.020157  0.063171  0.338623 -0.086243  0.288086  negative  \n",
       "60933  -0.363281 -0.171265 -0.366943 -0.346680  0.191772  0.468506  negative  \n",
       "\n",
       "[10 rows x 101 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from embeddings.textual_representation import AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\")\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, aggregate_method=\"avg\", \n",
    "                                            words_to_filter=stopwords.words('english'), words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "emb_keywords_exp.preprocess_train_dataset(df_amazon_mini, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the\n",
      "10000: persecution\n",
      "20000: baths\n",
      "30000: mortally\n",
      "40000: 1667\n",
      "50000: bec\n",
      "60000: baek\n",
      "70000: b/w\n",
      "80000: klinghoffer\n",
      "90000: azarov\n",
      "100000: capron\n",
      "110000: perpetua\n",
      "120000: biratnagar\n",
      "130000: 12.74\n",
      "140000: yaffa\n",
      "150000: cryogenics\n",
      "160000: ef1\n",
      "170000: franchetti\n",
      "180000: blintzes\n",
      "190000: birthstones\n",
      "200000: naadam\n",
      "210000: concertation\n",
      "220000: lesticus\n",
      "230000: containerboard\n",
      "240000: boydston\n",
      "250000: afterellen.com\n",
      "260000: acuff-rose\n",
      "270000: close-fitting\n",
      "280000: packbot\n",
      "290000: comptel\n",
      "300000: tanke\n",
      "310000: saraju\n",
      "320000: rouiba\n",
      "330000: discomfit\n",
      "340000: numurkah\n",
      "350000: hla-a\n",
      "360000: 90125\n",
      "370000: zipkin\n",
      "380000: lombarde\n",
      "390000: 1.137\n",
      "Palavras ignoradas: 0\n"
     ]
    }
   ],
   "source": [
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "\n",
    "emotion_words = {\n",
    "                    \"pride\":{\"proud\"},\n",
    "                    \"elation\":{\"ecstatic\", \"euphoria\", \"exaltation\", \"exhilarating\"},#vs boredom\n",
    "                    \"happiness\":{\"joy\",\"cheer\", \"bliss\", \"delight\", \"enjoy\", \"happy\"},#vs sad\n",
    "                    \"satisfaction\":{\"comfortable\",\"contentment\"},#\n",
    "                    \"relief\":{},\n",
    "                    \"hope\":{\"buoyancy\", \"confident\", \"faith\", \"optimistic\"},\n",
    "                    \"interest\":{\"alert\", \"animation\", \"ardor\", \"curious\",\"enthusiasm\"},\n",
    "                    \"surprise\":{\"amazed\", \"astonishing\", \"dumbfounded\",\"thunderstruck\"},\n",
    "                    \"anxiety\":{\"anguish\",\"anxiety\",\"apprehensive\",\"jittery\",\"nervous\",\"worry\"},\n",
    "                    \"sadness\":{\"chagrin\", \"dejected\", \"gloom\", \"hopeless\", \"melancholy\", \"sad\", \"tear\"},\n",
    "                    \"boredom\":{\"ennui\",\"indifference\",\"tedious\"},\n",
    "                    \"shame\":{\"abashed\", \"ashamed\", \"embarrassing\", \"humiliating\"},\n",
    "                    \"guilt\":{\"blame\", \"contrition\", \"remorse\"},\n",
    "                    \"disgust\":{\"abhor\", \"aversion\", \"dislike\", \"disrelish\", \"nausea\",\"sick\"},\n",
    "                    \"contempt\":{\"denigration\",\"depreciate\",\"derision\",\"disdain\",\"scorn\"},\n",
    "                    \"hostile\":{},\n",
    "                    \"anger\":{\"anger\",\"angry\",\"furious\",\"fury\",\"incense\",\"infuriating\",\n",
    "                                \"mad\",\"rage\",\"resent\",\"temper\",\"wrath\"},\n",
    "                    \"recognition\":{\"respect\",\"acknowledgement\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.en.100.txt\") \n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_en.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Representações usadas**:Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words)\n",
    "\n",
    "arr_representations = [embedding,emb_nostop, emb_keywords_exp, bow,bow_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204215</th>\n",
       "      <td>Do NOT WASTE Your Time: This book, to put it n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208138</th>\n",
       "      <td>Peels the paint off the walls: I first heard t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157010</th>\n",
       "      <td>History With Modern Appeal: This is a must rea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274316</th>\n",
       "      <td>Worse Music cd ever: I tried putting this in a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57708</th>\n",
       "      <td>Deliberately Obtuse Nonsense: I don't know wha...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29215</th>\n",
       "      <td>Better than the movie?: YES! This book gets be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256457</th>\n",
       "      <td>The Best RE yet: This is the best in the RE se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210215</th>\n",
       "      <td>What are they waiting for?: This has got to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200693</th>\n",
       "      <td>Hollywood - promoting the Antichrist again?: C...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169551</th>\n",
       "      <td>Best American TV Series Ever: With its combina...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     class\n",
       "id                                                                 \n",
       "204215  Do NOT WASTE Your Time: This book, to put it n...  negative\n",
       "208138  Peels the paint off the walls: I first heard t...  positive\n",
       "157010  History With Modern Appeal: This is a must rea...  positive\n",
       "274316  Worse Music cd ever: I tried putting this in a...  negative\n",
       "57708   Deliberately Obtuse Nonsense: I don't know wha...  negative\n",
       "...                                                   ...       ...\n",
       "29215   Better than the movie?: YES! This book gets be...  positive\n",
       "256457  The Best RE yet: This is the best in the RE se...  positive\n",
       "210215  What are they waiting for?: This has got to be...  positive\n",
       "200693  Hollywood - promoting the Antichrist again?: C...  negative\n",
       "169551  Best American TV Series Ever: With its combina...  positive\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import calcula_experimento_representacao, OtimizacaoObjetivoRandomForest\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\"random_forest\":{\"classe_otimizacao\":OtimizacaoObjetivoRandomForest,\n",
    "                                \"sampler\":optuna.samplers.TPESampler(seed=1, n_startup_trials=10)},\n",
    "              }\n",
    "df_amazon_reviews = pd.read_csv(\"datasets/amazon_reviews_mini.txt\",index_col=\"id\")\n",
    "\n",
    "#executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        col_classe = \"class\"\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 100\n",
    "\n",
    "        nom_experimento = f\"{metodo}_\"+representation.nome\n",
    "        experimento = calcula_experimento_representacao(nom_experimento,representation,df_amazon_reviews,\n",
    "                                            col_classe,num_folds,num_folds_validacao,num_trials,\n",
    "                                            ClasseObjetivoOtimizacao=param_metodo['classe_otimizacao'],\n",
    "                                                sampler=param_metodo['sampler'])\n",
    "        print(f\"Representação: {representation.nome} concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "pd.DataFrame.from_dict(arr_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
